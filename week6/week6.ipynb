{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqM5Z7qKUgqS"
      },
      "source": [
        "In this week, you are required to implement a toy GATConv and SAGEConv based on document. Also, you need to implement both in PyG and DGL. In this work, you will get a further understanding of tensor-centric in PyG and graph-centric in DGL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PYG 复现GATConv和SAGEConv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torch_geometric.utils import add_self_loops, degree\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import add_self_loops, softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "mclJkxgsUbRY"
      },
      "outputs": [],
      "source": [
        "class PyG_GATConv(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(PyG_GATConv, self).__init__(aggr='mean')\n",
        "        self.lin = nn.Linear(in_channels, out_channels)\n",
        "        self.att = nn.Parameter(torch.Tensor(1, 2 * out_channels))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.lin.weight)\n",
        "        nn.init.xavier_uniform_(self.att)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
        "\n",
        "        x = self.lin(x)\n",
        "\n",
        "        return self.propagate(edge_index, x=x)\n",
        "\n",
        "    def message(self, x_i, x_j, edge_index_i, num_nodes):\n",
        "        alpha = torch.cat([x_i, x_j], dim=-1)\n",
        "        alpha = (alpha * self.att).sum(dim=-1)\n",
        "        alpha = F.leaky_relu(alpha, negative_slope=0.2)\n",
        "        alpha = softmax(alpha, edge_index_i, num_nodes)\n",
        "\n",
        "        return x_j * alpha.view(-1, 1)\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        return aggr_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PyG_SAGEConv(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(PyG_SAGEConv, self).__init__(aggr='mean')\n",
        "        self.lin = nn.Linear(in_channels, out_channels)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.lin.weight)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
        "\n",
        "        x = self.lin(x)\n",
        "\n",
        "        return self.propagate(edge_index, x=x)\n",
        "\n",
        "    def message(self, x_j):\n",
        "        return x_j\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        return aggr_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.4938,  0.6922, -0.7242,  0.0556],\n",
            "        [-0.9877,  1.3844, -1.4483,  0.1112],\n",
            "        [-0.3292,  0.4615, -0.4828,  0.0371],\n",
            "        [-0.3292,  0.4615, -0.4828,  0.0371],\n",
            "        [-0.4938,  0.6922, -0.7242,  0.0556]], grad_fn=<DivBackward0>)\n",
            "tensor([[ 0.0930, -3.0377,  0.0126, -1.6848],\n",
            "        [ 0.0930, -3.0377,  0.0126, -1.6848],\n",
            "        [ 0.0930, -3.0377,  0.0126, -1.6848],\n",
            "        [ 0.0930, -3.0377,  0.0126, -1.6848],\n",
            "        [ 0.0930, -3.0377,  0.0126, -1.6848]], grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "source": [
        "edge_index = torch.tensor([[0,1,1,2,2,4],[2,0,2,3,4,3]])\n",
        "x = torch.ones((5, 8))\n",
        "conv = PyG_GATConv(8, 4)\n",
        "output = conv(x, edge_index)\n",
        "print(output)\n",
        "conv = PyG_SAGEConv(8, 4)\n",
        "output = conv(x, edge_index)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DGL 复现GATConv和SAGEConv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import dgl\n",
        "import dgl.function as fn\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from dgl.utils import DGLError\n",
        "from dgl.utils import check_eq_shape, expand_as_pair\n",
        "from dgl.nn import SAGEConv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "HbdLhvBSVYEr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class DGL_SAGEConv(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats, aggregator_type='mean'):\n",
        "        super(DGL_SAGEConv, self).__init__()\n",
        "        self.aggregator_type = aggregator_type\n",
        "        self.fc = nn.Linear(in_feats * 2, out_feats)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.norm = nn.LayerNorm(out_feats)\n",
        "\n",
        "    def forward(self, graph, h):\n",
        "        #聚合\n",
        "        with graph.local_scope():\n",
        "            if self.aggregator_type == 'mean':\n",
        "                graph.ndata['h'] = h\n",
        "                graph.update_all(fn.copy_u('h', 'm'), fn.mean('m', 'h_neigh'))\n",
        "            elif self.aggregator_type == 'gcn':\n",
        "                degs = graph.in_degrees().float().clamp(min=1)\n",
        "                norm = torch.pow(degs, -0.5)\n",
        "                norm = norm.to(h.device).unsqueeze(1)\n",
        "                graph.ndata['h'] = h * norm\n",
        "                graph.update_all(fn.copy_u('h', 'm'), fn.sum('m', 'h_neigh'))\n",
        "                graph.ndata['h_neigh'] = graph.ndata['h_neigh'] * norm\n",
        "            elif self.aggregator_type == 'pool' or self.aggregator_type == 'lstm':\n",
        "                raise NotImplementedError\n",
        "            else:\n",
        "                raise KeyError('Aggregator type {} not recognized.'.format(self.aggregator_type))\n",
        "\n",
        "            h_neigh = graph.ndata['h_neigh']\n",
        "            h_concat = torch.cat([h, h_neigh], dim=1)\n",
        "            h_prime = self.fc(h_concat)\n",
        "            \n",
        "            #ReLU  norm\n",
        "            h_prime = self.relu(h_prime)\n",
        "            h_prime = self.norm(h_prime)\n",
        "            \n",
        "            return h_prime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dgl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DGL_GATConv(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel):\n",
        "        super(DGL_GATConv, self).__init__()\n",
        "        # 定义可训练的权重\n",
        "        self.fc = nn.Linear(in_channel, out_channel, bias=False)\n",
        "        # 注意力权重\n",
        "        self.attn_fc = nn.Linear(2*out_channel, 1, bias=False)\n",
        "\n",
        "    def edge_attention(self, edges):\n",
        "        # 计算边的注意力分数\n",
        "        z2 = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n",
        "        a = self.attn_fc(z2)\n",
        "        return {'e': F.leaky_relu(a)}\n",
        "\n",
        "    def message_func(self, edges):\n",
        "        # 应用注意力权重\n",
        "        return {'z': edges.src['z'], 'e': edges.data['e']}\n",
        "\n",
        "    def reduce_func(self, nodes):\n",
        "        # 聚合邻居节点的特征\n",
        "        alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
        "        h = torch.sum(alpha * nodes.mailbox['z'], dim=1)\n",
        "        return {'h': h}\n",
        "\n",
        "    def forward(self, g, h):\n",
        "        # 应用线性变换\n",
        "        z = self.fc(h)\n",
        "        g.ndata['z'] = z\n",
        "\n",
        "        # 计算注意力\n",
        "        g.apply_edges(self.edge_attention)\n",
        "\n",
        "        # 聚合信息\n",
        "        g.update_all(self.message_func, self.reduce_func)\n",
        "\n",
        "        # 获取最终的特征表示\n",
        "        return g.ndata.pop('h')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.1723, -0.2970, -0.4610,  0.3394],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.1723, -0.2970, -0.4610,  0.3394],\n",
            "        [-0.1723, -0.2970, -0.4610,  0.3394],\n",
            "        [-0.1723, -0.2970, -0.4610,  0.3394]], grad_fn=<IndexCopyBackward0>)\n",
            "tensor([[-1.0420,  0.2558,  1.5199, -0.7337],\n",
            "        [-0.8681, -0.4498,  1.7018, -0.3840],\n",
            "        [-1.0420,  0.2558,  1.5199, -0.7337],\n",
            "        [-1.0420,  0.2558,  1.5199, -0.7337],\n",
            "        [-1.0420,  0.2558,  1.5199, -0.7337]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ],
      "source": [
        "src = torch.tensor([0, 1, 1, 2, 2, 4])\n",
        "dst = torch.tensor([2, 0, 2, 3, 4, 3])\n",
        "h = torch.ones((5, 8))\n",
        "g = dgl.graph((src, dst))\n",
        "\n",
        "conv = DGL_GATConv(8, 4)\n",
        "output = conv(g, h)\n",
        "print(output)\n",
        "\n",
        "conv = DGL_SAGEConv(8, 4)\n",
        "output = conv(g, h)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GraphConv问题回答：\n",
        "\n",
        "(1)公式对应：  \n",
        "消息部分：mailBox=(eij/cij)*W*hj  \n",
        "聚合部分：把所有消息进行求和  \n",
        "更新部分：加上偏差并通过激活函数进行更新  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GATConv问题回答：\n",
        "\n",
        "（1）公式对应：   \n",
        "消息:W*h_j   \n",
        "聚合：先求出注意力系数 \\e_ij\\,然后,按照exp(e_ij)进行分配权重，按照权重，对邻居特征，进行特征求和   \n",
        "更新:激活函数  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SAGEConv问题回答："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "（1）norm对应:  \n",
        "公式第一步：利用邻居和自己的特征（如果边特征给出也包括在内）线性变换，并分别和i的特征concat在一起 并实现聚合  \n",
        "公式第二部：将聚合后的特征和W参数进行操作，更新  \n",
        "公式第三部：按照给定的norm范式，再次进行更新  \n",
        "\n",
        "（2）concat对应：\n",
        "\n",
        "在源码中：rst = self.fc_self(h_self) + h_neigh  \n",
        "是将线性变换后 自身节点的特征和 线性变换后邻居节点的特征 进行相加；而不是连接\n",
        "\n",
        "所以对于W参数而言，在源码中的方式，shape为(feature_in,feature_out)\n",
        "\n",
        "但是，如果按照concat连接的方式，则concat后的矩阵为w*feature_in维的向量，W也应该变为(2*feature_in,feature_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
